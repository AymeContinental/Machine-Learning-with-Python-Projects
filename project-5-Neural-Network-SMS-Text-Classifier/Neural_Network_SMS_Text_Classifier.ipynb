{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXNGDHFQXInC",
        "outputId": "ce0c903a-c871-4ad6-fdbd-9d55bbe9abba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Datos cargados correctamente.\n",
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# Celda 1: Importar librerías y cargar datos\n",
        "# ============================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Los datasets ya vienen cargados en el entorno del reto,\n",
        "# pero si estás en Colab, puedes subirlos manualmente o usar una URL.\n",
        "# En este ejemplo, los cargamos desde URLs públicas:\n",
        "train_data = pd.read_csv(\"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\", sep='\\t', header=None, names=['label', 'message'])\n",
        "test_data = train_data.sample(frac=0.2, random_state=42)\n",
        "train_data = train_data.drop(test_data.index)\n",
        "\n",
        "print(\"✅ Datos cargados correctamente.\")\n",
        "print(train_data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Celda 2: Preparar datos\n",
        "# ============================================\n",
        "\n",
        "# Convertir etiquetas \"ham\"/\"spam\" a 0/1\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_data[\"label\"])\n",
        "test_labels = label_encoder.transform(test_data[\"label\"])\n",
        "\n",
        "# Tokenizar texto (convertir palabras a enteros)\n",
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_length = 120\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_data[\"message\"])\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data[\"message\"])\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data[\"message\"])\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(\"✅ Textos tokenizados y preparados.\")\n",
        "print(f\"Ejemplo de secuencia: {train_sequences[0][:10]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozQyBnwqXaL9",
        "outputId": "f855815d-633e-4e9d-a86e-cfab9daeda93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Textos tokenizados y preparados.\n",
            "Ejemplo de secuencia: [50, 576, 1, 1, 748, 600, 66, 9, 1, 86]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Celda 3: Crear y entrenar el modelo\n",
        "# ============================================\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_padded,\n",
        "    np.array(train_labels),\n",
        "    epochs=10,\n",
        "    validation_data=(test_padded, np.array(test_labels)),\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(\"✅ Modelo entrenado correctamente.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w53HX1HRXbNu",
        "outputId": "5996fc1b-cc37-4907-bab3-ae5739cd25f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140/140 - 2s - 15ms/step - accuracy: 0.8656 - loss: 0.3853 - val_accuracy: 0.8662 - val_loss: 0.3648\n",
            "Epoch 2/10\n",
            "140/140 - 1s - 4ms/step - accuracy: 0.8654 - loss: 0.3576 - val_accuracy: 0.8654 - val_loss: 0.3490\n",
            "Epoch 3/10\n",
            "140/140 - 1s - 4ms/step - accuracy: 0.8654 - loss: 0.3334 - val_accuracy: 0.8591 - val_loss: 0.3156\n",
            "Epoch 4/10\n",
            "140/140 - 1s - 4ms/step - accuracy: 0.8699 - loss: 0.2782 - val_accuracy: 0.8968 - val_loss: 0.2528\n",
            "Epoch 5/10\n",
            "140/140 - 1s - 4ms/step - accuracy: 0.9098 - loss: 0.2072 - val_accuracy: 0.9093 - val_loss: 0.1927\n",
            "Epoch 6/10\n",
            "140/140 - 1s - 4ms/step - accuracy: 0.9457 - loss: 0.1581 - val_accuracy: 0.9372 - val_loss: 0.1526\n",
            "Epoch 7/10\n",
            "140/140 - 1s - 4ms/step - accuracy: 0.9542 - loss: 0.1299 - val_accuracy: 0.9515 - val_loss: 0.1230\n",
            "Epoch 8/10\n",
            "140/140 - 1s - 4ms/step - accuracy: 0.9677 - loss: 0.1043 - val_accuracy: 0.9722 - val_loss: 0.1140\n",
            "Epoch 9/10\n",
            "140/140 - 1s - 6ms/step - accuracy: 0.9706 - loss: 0.0876 - val_accuracy: 0.9632 - val_loss: 0.0966\n",
            "Epoch 10/10\n",
            "140/140 - 1s - 9ms/step - accuracy: 0.9751 - loss: 0.0744 - val_accuracy: 0.9811 - val_loss: 0.0703\n",
            "✅ Modelo entrenado correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Celda 4: Función predict_message\n",
        "# ============================================\n",
        "\n",
        "def predict_message(message):\n",
        "    seq = tokenizer.texts_to_sequences([message])\n",
        "    padded = pad_sequences(seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "    prediction = model.predict(padded, verbose=0)[0][0]\n",
        "    label = \"spam\" if prediction > 0.5 else \"ham\"\n",
        "    return [float(prediction), label]\n"
      ],
      "metadata": {
        "id": "iaMBwgqSXcsF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Celda 5: Prueba final\n",
        "# ============================================\n",
        "\n",
        "tests = [\n",
        "    \"Hey, are we still meeting for dinner tonight?\",\n",
        "    \"Congratulations! You've been selected for a $1000 Walmart gift card. Go to http://bit.ly/12345 to claim now.\",\n",
        "    \"Don't forget to bring the documents for tomorrow's meeting.\",\n",
        "    \"You won a free cruise to the Bahamas! Call now to claim your prize.\",\n",
        "]\n",
        "\n",
        "for msg in tests:\n",
        "    print(msg)\n",
        "    print(predict_message(msg))\n",
        "    print(\"-----\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw6k-JdLXdxZ",
        "outputId": "f413b85a-8095-4999-fafa-b022b14d19fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey, are we still meeting for dinner tonight?\n",
            "[0.008391112089157104, 'ham']\n",
            "-----\n",
            "Congratulations! You've been selected for a $1000 Walmart gift card. Go to http://bit.ly/12345 to claim now.\n",
            "[0.7261187434196472, 'spam']\n",
            "-----\n",
            "Don't forget to bring the documents for tomorrow's meeting.\n",
            "[0.038604311645030975, 'ham']\n",
            "-----\n",
            "You won a free cruise to the Bahamas! Call now to claim your prize.\n",
            "[0.8994461894035339, 'spam']\n",
            "-----\n"
          ]
        }
      ]
    }
  ]
}